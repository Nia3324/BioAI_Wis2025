{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20fd777",
   "metadata": {},
   "source": [
    "## Creating Testing Dataset for Baseline Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b70195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import math\n",
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
    "from pathlib import Path\n",
    "import whisper\n",
    "import copy, heapq\n",
    "from transformers import AutoFeatureExtractor, WhisperModel\n",
    "from transformers import LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch, torchaudio\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from jiwer import wer as calculate_wer\n",
    "import pickle\n",
    "from datasets import Dataset, Audio, Value\n",
    "import os, random\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a429cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(csv_path, file_list_path, feature_extractor, origin, tokenizer=None, base_audio_dir=\"\", max_label_length=None):\n",
    "    df_text = pd.read_csv(csv_path)\n",
    "\n",
    "    with open(file_list_path, \"r\") as f:\n",
    "        file_paths = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    dataset = []\n",
    "    i = 0\n",
    "    for file_path in file_paths:\n",
    "        row = df_text[df_text[\"path\"] == file_path]\n",
    "        if row.empty:\n",
    "            print(f\"[WARN] No transcript found for: {file_path}\")\n",
    "            continue\n",
    "        text = str(row[\"sentence\"].values[0]).lower().strip()\n",
    "\n",
    "        full_audio_path = os.path.join(base_audio_dir, file_path)\n",
    "\n",
    "        audio, sr = torchaudio.load(full_audio_path)\n",
    "        if sr != 16000:\n",
    "            audio = torchaudio.functional.resample(audio, sr, 16000)\n",
    "\n",
    "        feats = feature_extractor(\n",
    "            audio.squeeze(0).numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_features\"]\n",
    "\n",
    "        item = {\"input_features\": feats, \"text\": text}\n",
    "\n",
    "        if tokenizer is not None:\n",
    "            labels = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=max_label_length\n",
    "            )[\"input_ids\"][0]\n",
    "            item[\"labels\"] = labels\n",
    "        i += 1\n",
    "        print(\"Missing:\", i / len(file_paths) * 100, \"%\")\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def compute_confidence_score(generate_outputs, generated_ids):\n",
    "\n",
    "    log_probs_list = []\n",
    "\n",
    "    for step_logits, token_id in zip(generate_outputs.scores, generated_ids[0, 1:]):\n",
    "        log_probs = F.log_softmax(step_logits, dim=-1)\n",
    "        token_log_prob = log_probs[0, token_id]\n",
    "        log_probs_list.append(token_log_prob.item())\n",
    "    avg_log_prob = sum(log_probs_list) / len(log_probs_list)\n",
    "    \n",
    "    return avg_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
    "processor = WhisperProcessor.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "normalizer = EnglishTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_csv = '/content/drive/MyDrive/data/final_test.csv'\n",
    "test_audio_list = '/content/drive/MyDrive/data/test_files.txt'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "initial_model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n",
    "max_label_length = initial_model.config.max_target_positions\n",
    "del initial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cff76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_set = pd.read_csv('/content/drive/MyDrive/data/final_test.csv')\n",
    "df_train_set = pd.read_csv('/content/drive/MyDrive/data/final_train.csv')\n",
    "\n",
    "accents = df_test_set['accent'].unique().tolist() + df_train_set['accent'].unique().tolist()\n",
    "accents = list(set(accents))\n",
    "\n",
    "for i in accents:\n",
    "    new_df_test = df_test_set[df_test_set['accent']==i]\n",
    "    new_df_test.to_csv(f'/content/drive/MyDrive/data/test_{i}.csv', index=False)\n",
    "\n",
    "    df_prep = data_preparation(f'/content/drive/MyDrive/data/test.csv', test_audio_list, feature_extractor, tokenizer, base_audio_dir='/content/drive/MyDrive/data/test_data', origin='test', max_label_length=max_label_length)\n",
    "    torch.save(df_prep, f'{i}/test.pt')\n",
    "\n",
    "    new_df_train = df_train_set[df_train_set['accent']==i]\n",
    "    new_df_train.to_csv(f'/content/drive/MyDrive/data/{i}/train.csv', index=False)\n",
    "\n",
    "    df_prep_train = data_preparation(f'/content/drive/MyDrive/data/{i}/train.csv', test_audio_list, feature_extractor, tokenizer, base_audio_dir='/content/drive/MyDrive/data/train_data', origin='train', max_label_length=max_label_length)\n",
    "    torch.save(df_prep_train, f'{i}/train.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f6957d",
   "metadata": {},
   "source": [
    "## Creating Unlabaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pseudo_labels(df_unlabeled, model_frozen, feature_extractor, tokenizer, device, base_dirs, confidence_threshold=0.85):\n",
    "    pseudo_records = []\n",
    "\n",
    "    for path in df_unlabeled[\"path\"].tolist():\n",
    "        full_path = None\n",
    "        for d in base_dirs:\n",
    "            candidate = os.path.join(d, path)\n",
    "            if os.path.exists(candidate):\n",
    "                full_path = candidate\n",
    "                break\n",
    "\n",
    "        if full_path is None:\n",
    "            if os.path.exists(path):\n",
    "                full_path = path\n",
    "            else:\n",
    "                print(f\"[WARN] audio file not found for: {path}\")\n",
    "                continue\n",
    "\n",
    "        audio, sr = torchaudio.load(full_path)\n",
    "        if sr != 16000:\n",
    "            audio = torchaudio.functional.resample(audio, sr, 16000)\n",
    "\n",
    "        audio_np = audio.squeeze(0).numpy()\n",
    "\n",
    "        inputs = feature_extractor(\n",
    "            audio_np,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_features = inputs[\"input_features\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_frozen.generate(\n",
    "                input_features,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "            generated_ids = outputs.sequences\n",
    "            confidence = compute_confidence_score(outputs, generated_ids)\n",
    "\n",
    "        if confidence < confidence_threshold:\n",
    "            print(f\"[INFO] Low confidence ({confidence:.3f}) for: {path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        transcription = tokenizer.batch_decode(\n",
    "            generated_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0].lower().strip()\n",
    "\n",
    "        pseudo_records.append({\"path\": path, \"sentence\": transcription})\n",
    "\n",
    "    df_pseudo = pd.DataFrame(pseudo_records)\n",
    "    df_pseudo[\"source\"] = \"pseudo\"\n",
    "\n",
    "    return df_pseudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceacad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frozen = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n",
    "model_frozen.to(device)\n",
    "\n",
    "for accent in accents:\n",
    "    df = pd.read_csv('/content/drive/MyDrive/data/final_train.csv')\n",
    "    df_train = pd.read_csv(f'/content/drive/MyDrive/data/{accent}/train.csv')\n",
    "    df_unlabeled = df_train.sample(frac=0.3, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    base_dirs = ['/content/drive/MyDrive/data/final_train.csv']\n",
    "\n",
    "    pseudo_labels_df = generate_pseudo_labels(df_unlabeled, model_frozen, feature_extractor, tokenizer, device, base_dirs, confidence_threshold=0.85)\n",
    "\n",
    "    pseudo_labels_df.to_csv(f'/content/drive/MyDrive/data/{accent}/df_pseudo_training.csv', index=False)\n",
    "\n",
    "    df_pseudo_training = pd.read_csv(f'/content/drive/MyDrive/data/{accent}/df_pseudo_training.csv')\n",
    "    for idx, row in df_pseudo_training.iterrows():\n",
    "        path = row['path']\n",
    "        pseudo_sentence = row['sentence']\n",
    "        df.loc[df[\"path\"] == path, \"sentence\"] = pseudo_sentence\n",
    "\n",
    "    df.to_csv(f'/content/drive/MyDrive/data/{accent}/df_pseudo_training.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_for_len = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n",
    "max_label_length = model_config_for_len.config.max_target_positions\n",
    "del model_config_for_len\n",
    "\n",
    "for accent in accents:\n",
    "    df_pseudo_training = pd.read_csv(f'/content/drive/MyDrive/data/{accent}/df_pseudo_training.csv')\n",
    "    train_dataset = data_preparation(f'/content/drive/MyDrive/data/{accent}/df_pseudo_training.csv', test_audio_list, feature_extractor, tokenizer, base_audio_dir='/content/drive/MyDrive/data/train_data', origin='train', max_label_length=max_label_length)\n",
    "    torch.save(train_dataset, f'{accent}/df_pseudo_training.pt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "db4bceaf",
      "metadata": {},
      "source": [
        "## STAR Algorithm Adaptation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88593b4",
      "metadata": {
        "id": "b88593b4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from jiwer import wer\n",
        "from whisper.normalizers import EnglishTextNormalizer\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import logging\n",
        "from transformers import (\n",
        "    WhisperForConditionalGeneration,\n",
        "    WhisperProcessor,\n",
        "    WhisperTokenizer,\n",
        "    WhisperFeatureExtractor\n",
        ")\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NORMALIZER = EnglishTextNormalizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ydPXlSmvcQKY",
      "metadata": {
        "id": "ydPXlSmvcQKY"
      },
      "outputs": [],
      "source": [
        "# ================= CONFIGURATION =================\n",
        "TRAIN_AUDIO = \"data/train_data\"\n",
        "TRAIN_METADATA = \"data/final_train.csv\"\n",
        "\n",
        "TEST_AUDIO = \"data/test_data\"\n",
        "TEST_METADATA = \"data/final_test.csv\"\n",
        "\n",
        "OUTPUT_TRAIN = \"data/star_process/train_all.pt\"\n",
        "OUTPUT_FULL = \"data/star_process/train_final_ready.pt\"\n",
        "\n",
        "accents = [\"India and South Asia (India, Pakistan, Sri Lanka)\",\n",
        "           \"Australian English\",\n",
        "           \"Southern African (South Africa, Zimbabwe, Namibia)\",\n",
        "           \"Singaporean English\"]\n",
        "\n",
        "TEST_IN = \"data/test/test_in.pt\"\n",
        "TEST_AU = \"data/test/test_au.pt\"\n",
        "TEST_SA = \"data/test/test_sa.pt\"\n",
        "TEST_SG = \"data/test/test_sg.pt\"\n",
        "TEST_PATH= \"data/test\"\n",
        "COL_FILENAME = \"path\"\n",
        "COL_TEXT = \"sentence\"\n",
        "# ================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uV8JVZ3gQ3C-",
      "metadata": {
        "id": "uV8JVZ3gQ3C-"
      },
      "source": [
        "## Get torch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dhFpx1ItTHMj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhFpx1ItTHMj",
        "outputId": "fde8b91d-caa4-436f-81fe-48ad5023e1af"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main():\n",
        "    os.makedirs(os.path.dirname(OUTPUT_TRAIN), exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(TRAIN_METADATA):\n",
        "        print(f\"Error: Could not find CSV at {TRAIN_METADATA}\")\n",
        "        return\n",
        "    if not os.path.exists(TEST_METADATA):\n",
        "        print(f\"Error: Could not find CSV at {TEST_METADATA}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(TRAIN_METADATA)\n",
        "    df_test = pd.read_csv(TEST_METADATA)\n",
        "    print(f\"Found {len(df)} rows in train CSV.\")\n",
        "    print(f\"Found {len(df_test)} rows in test CSV.\")\n",
        "\n",
        "    dataset_list = []\n",
        "    test_list = []\n",
        "\n",
        "    print(f\"Processing audio from: {TRAIN_AUDIO}...\")\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        filename = str(row[COL_FILENAME])\n",
        "        text = row[COL_TEXT]\n",
        "\n",
        "        file_path = os.path.join(TRAIN_AUDIO, filename)\n",
        "\n",
        "        # Handle missing .mp3 extension if CSV doesn't have it\n",
        "        if not os.path.exists(file_path):\n",
        "            if os.path.exists(file_path + \".mp3\"):\n",
        "                file_path += \".mp3\"\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        try:\n",
        "            audio_array, _ = librosa.load(file_path, sr=16000)\n",
        "            entry = {\n",
        "                \"audio\": {\n",
        "                    \"array\": audio_array,\n",
        "                    \"sampling_rate\": 16000\n",
        "                },\n",
        "                \"text\": text,\n",
        "                \"file_path\": file_path\n",
        "            }\n",
        "            dataset_list.append(entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {filename}: {e}\")\n",
        "\n",
        "    # process each test accent separately\n",
        "    for accent, output_file in zip(accents, [TEST_IN, TEST_AU, TEST_SA, TEST_SG]):\n",
        "        accent_test_list = []\n",
        "        print(f\"Processing test audio for accent: {accent}...\")\n",
        "        for idx, row in tqdm(df_test[df_test['accents'] == accent].iterrows(), total=len(df_test[df_test['accents'] == accent])):\n",
        "            filename = str(row[COL_FILENAME])\n",
        "            text = row[COL_TEXT]\n",
        "\n",
        "            file_path = os.path.join(TEST_AUDIO, filename)\n",
        "\n",
        "            # Handle missing .mp3 extension if CSV doesn't have it\n",
        "            if not os.path.exists(file_path):\n",
        "                if os.path.exists(file_path + \".mp3\"):\n",
        "                    file_path += \".mp3\"\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            try:\n",
        "                audio_array, _ = librosa.load(file_path, sr=16000)\n",
        "                entry = {\n",
        "                    \"audio\": {\n",
        "                        \"array\": audio_array,\n",
        "                        \"sampling_rate\": 16000\n",
        "                    },\n",
        "                    \"text\": text,\n",
        "                    \"file_path\": file_path\n",
        "                }\n",
        "                accent_test_list.append(entry)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load {filename}: {e}\")\n",
        "\n",
        "        # save accent-specific test set\n",
        "        print(f\"Saving test dataset for accent {accent} to {output_file}...\")\n",
        "        torch.save(accent_test_list, output_file)\n",
        "        test_list.extend(accent_test_list)\n",
        "    print(f\"Total test samples processed: {len(test_list)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exWfpusQsQgU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "exWfpusQsQgU",
        "outputId": "c2e7b2de-3541-423e-8e34-a15be3c621a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'audio': {'array': array([-1.0339758e-23, -2.6883370e-23, -8.2718061e-24, ...,\n",
              "         -2.5170762e-04, -2.0353909e-04, -6.1307161e-05],\n",
              "        shape=(65280,), dtype=float32),\n",
              "  'sampling_rate': 16000},\n",
              " 'text': 'She told me she feels hopeful and optimistic.',\n",
              " 'file_path': 'data/train_data/common_voice_en_134463.mp3'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# preview train_all.pt\n",
        "train_all = torch.load(\"data/star_ready/train_all.pt\", weights_only=False)\n",
        "train_all[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14930ee4",
      "metadata": {
        "id": "14930ee4"
      },
      "source": [
        "## Get Pseudo-Labels and Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UZ8bipVsA0-r",
      "metadata": {
        "id": "UZ8bipVsA0-r"
      },
      "source": [
        "**Training Configuration:**\n",
        "- Optimizer: Adam\n",
        "- Learning Rate: Initial rate of $1 \\times 10^{-5}$\n",
        "- Duration: 2 epochs\n",
        "- Batch Strategy: Batch size of 1 with 16 gradient accumulation steps\n",
        "\n",
        "**Hyperparameters & Filtering:**\n",
        "- Threshold ($\\lambda$): 2\n",
        "- Temperature ($\\tau$): 10\n",
        "- Utterance-level Filtering ($\\alpha$): 20th percentile (chosen for consistent effectiveness across datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bcac801",
      "metadata": {
        "id": "1bcac801"
      },
      "outputs": [],
      "source": [
        "# ================= CONFIGURATION =================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"openai/whisper-tiny\"\n",
        "DATA_PATH = \"data/\"\n",
        "STAR_PATH = \"data/star_process\"\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "# ================= CONFIGURATION =================\n",
        "INPUT_PT_FILE = \"data/train_all.pt\"\n",
        "OUTPUT_DIR = \"data/star_process\"\n",
        "MODEL_NAME = \"openai/whisper-tiny\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# STAR Hyperparameters\n",
        "THRESHOLD = 2.0\n",
        "TAU = 10\n",
        "TOP_PERCENT = 0.8\n",
        "NUM_CHUNKS = 4\n",
        "# =================================================\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def calculate_star_weights(probs, weights, threshold=THRESHOLD, tau=TAU):\n",
        "    \"\"\"Combines Confidence (probs) and Attentive Score (weights) [cite: 186-193].\"\"\"\n",
        "    final_scores = []\n",
        "    for ci, ai in zip(probs, weights):\n",
        "        ci = max(ci, 1e-6)\n",
        "        ai = max(ai, 1e-6)\n",
        "        c_over_a = (ci**2) / ai\n",
        "        a_over_c = (ai**2) / ci\n",
        "\n",
        "        conflict = (sigmoid((c_over_a - threshold) * tau) + sigmoid((a_over_c - threshold) * tau)) * ai\n",
        "        no_conflict = (sigmoid((threshold - c_over_a) * tau) * sigmoid((threshold - a_over_c) * tau)) * \\\n",
        "                      ai * np.exp((ci - ai) / tau)\n",
        "        final_scores.append(conflict + no_conflict)\n",
        "    return final_scores\n",
        "\n",
        "def prepare_star_dataset_chunked(model, input_file, processor, feature_extractor, tokenizer):\n",
        "    model.eval()\n",
        "\n",
        "    # Force configuration on the model object\n",
        "    # This ensures the generation loop cannot ignore these flags\n",
        "    model.config.output_attentions = True\n",
        "    model.config.output_scores = True\n",
        "    model.config.return_dict_in_generate = True\n",
        "\n",
        "    # Also update the generation_config if it exists\n",
        "    if model.generation_config:\n",
        "        model.generation_config.output_attentions = True\n",
        "        model.generation_config.output_scores = True\n",
        "        model.generation_config.return_dict_in_generate = True\n",
        "\n",
        "    # Get Prompt Info\n",
        "    forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
        "\n",
        "    print(f\"Loading raw data from {input_file}...\")\n",
        "    raw_data_list = torch.load(input_file, weights_only=False)\n",
        "\n",
        "    total_size = len(raw_data_list)\n",
        "    print(f\"Loaded {total_size} samples. Starting Fixed Processing...\")\n",
        "\n",
        "    chunk_size = (total_size + NUM_CHUNKS - 1) // NUM_CHUNKS\n",
        "    temp_files = []\n",
        "\n",
        "    # --- CHUNK LOOP ---\n",
        "    for chunk_idx in range(NUM_CHUNKS):\n",
        "        start = chunk_idx * chunk_size\n",
        "        end = min((chunk_idx + 1) * chunk_size, total_size)\n",
        "        if start >= total_size: break\n",
        "\n",
        "        print(f\"\\nProcessing Chunk {chunk_idx + 1}/{NUM_CHUNKS} (Samples {start} to {end})...\")\n",
        "        current_batch = raw_data_list[start:end]\n",
        "        chunk_data = []\n",
        "\n",
        "        # Identify params for perturbation once\n",
        "        params_to_noise = [p for p in model.parameters() if p.requires_grad and p.dtype == torch.float32]\n",
        "\n",
        "        for i, item in enumerate(tqdm(current_batch)):\n",
        "            try:\n",
        "                # --- 1. Audio Loading ---\n",
        "                if isinstance(item['audio'], dict):\n",
        "                    audio_array = item['audio']['array']\n",
        "                    sr = item['audio'].get('sampling_rate', 16000)\n",
        "                else:\n",
        "                    audio_array = item['audio']\n",
        "                    sr = 16000\n",
        "\n",
        "                if isinstance(audio_array, np.ndarray):\n",
        "                    audio_tensor = torch.from_numpy(audio_array).float()\n",
        "                else:\n",
        "                    audio_tensor = torch.tensor(audio_array).float()\n",
        "\n",
        "                if sr != 16000:\n",
        "                    audio_tensor = audio_tensor.cpu()\n",
        "                    resampler = torchaudio.transforms.Resample(sr, 16000)\n",
        "                    audio_tensor = resampler(audio_tensor.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "                audio_np = audio_tensor.cpu().numpy()\n",
        "                inputs = feature_extractor(audio_np, sampling_rate=16000, return_tensors=\"pt\")\n",
        "                mel = inputs.input_features.to(DEVICE)\n",
        "\n",
        "                # --- FIX: Create Attention Mask ---\n",
        "                # This prevents the \"attention mask not set\" warning\n",
        "                # Since we have batch size 1, the mask is just all ones\n",
        "                attention_mask = torch.ones(mel.shape[0], mel.shape[2], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "                # --- 2. FAST GENERATION ---\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(\n",
        "                        mel,\n",
        "                        # We pass mask explicitly\n",
        "                        # attention_mask=attention_mask, # Optional for Whisper, usually ignored for Encoder-Decoder but good practice\n",
        "                        max_new_tokens=128,\n",
        "                        # Arguments are now also backed by model.config\n",
        "                        output_attentions=True,\n",
        "                        output_scores=True,\n",
        "                        return_dict_in_generate=True,\n",
        "                        forced_decoder_ids=forced_decoder_ids\n",
        "                    )\n",
        "\n",
        "                # --- 3. Extract Confidence ---\n",
        "                if outputs.scores is None:\n",
        "                    # If this hits, the config fix didn't work, skip sample safely\n",
        "                    print(f\"WARNING: outputs.scores is None, skipping sample {i}.\")\n",
        "                    continue\n",
        "\n",
        "                scores_stack = torch.stack(outputs.scores, dim=0).squeeze(1)\n",
        "                probs_tensor = torch.softmax(scores_stack / 1.0, dim=-1)\n",
        "                top_probs, _ = probs_tensor.max(dim=-1)\n",
        "                probs = top_probs.tolist()\n",
        "\n",
        "                # --- 4. Reconstruct Attention Matrix ---\n",
        "                if outputs.decoder_attentions is None:\n",
        "                    print(f\"WARNING: outputs.decoder_attentions is None, skipping sample {i}.\")\n",
        "                    continue\n",
        "\n",
        "                generated_len = len(probs)\n",
        "\n",
        "                # Infer prompt length from first step\n",
        "                first_step_attn = outputs.decoder_attentions[0][-1]\n",
        "                total_len_start = first_step_attn.shape[-1]\n",
        "                n_prompt_toks = total_len_start - 1\n",
        "\n",
        "                full_matrix_len = n_prompt_toks + generated_len\n",
        "                attn_matrix = torch.zeros((generated_len, full_matrix_len))\n",
        "\n",
        "                for t in range(generated_len):\n",
        "                    step_attn = outputs.decoder_attentions[t][-1]\n",
        "                    # Average heads\n",
        "                    step_attn_avg = step_attn[0, :, 0, :].mean(dim=0).cpu()\n",
        "                    cur_len = step_attn_avg.shape[0]\n",
        "                    attn_matrix[t, :cur_len] = step_attn_avg\n",
        "\n",
        "                # --- 5. Calculate STAR Weights ---\n",
        "                weights = []\n",
        "                for k in range(generated_len):\n",
        "                    global_idx = n_prompt_toks + k\n",
        "                    #print(global_idx >= full_matrix_len)\n",
        "                    if global_idx >= full_matrix_len: break\n",
        "\n",
        "                    row_slice = attn_matrix[k, n_prompt_toks:]\n",
        "                    row_sum = row_slice.sum().item()\n",
        "\n",
        "                    col_slice = attn_matrix[:, global_idx]\n",
        "                    col_sum = col_slice.sum().item()\n",
        "\n",
        "                    diag = attn_matrix[k, global_idx].item()\n",
        "\n",
        "                    w = row_sum + col_sum - diag\n",
        "                    weights.append(float(w))\n",
        "\n",
        "                # Normalize & Combine\n",
        "                if len(probs) > 0 and len(weights) > 0:\n",
        "                    min_l = min(len(probs), len(weights))\n",
        "                    probs = probs[:min_l]\n",
        "                    weights = weights[:min_l]\n",
        "\n",
        "                    p_mean = np.mean(probs) if np.mean(probs) > 0 else 1e-6\n",
        "                    w_mean = np.mean(weights) if np.mean(weights) > 0 else 1e-6\n",
        "\n",
        "                    norm_probs = [p / p_mean for p in probs]\n",
        "                    norm_weights = [w / w_mean for w in weights]\n",
        "\n",
        "                    star_weights = calculate_star_weights(norm_probs, norm_weights)\n",
        "\n",
        "                    pred_ids = outputs.sequences[0]\n",
        "                    pseudo_text = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "                    # --- 6. Uncertainty (Fast) ---\n",
        "                    avg_wer = 0\n",
        "                    generated_texts = []\n",
        "\n",
        "                    for _ in range(3):\n",
        "                        noise_cache = []\n",
        "                        for p in params_to_noise:\n",
        "                            noise = torch.randn_like(p) * p.std() * 0.1\n",
        "                            p.data.add_(noise)\n",
        "                            noise_cache.append(noise)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            g_out = model.generate(mel, max_new_tokens=80)\n",
        "\n",
        "                            if isinstance(g_out, torch.Tensor):\n",
        "                                g_seq = g_out[0]\n",
        "                            else: # If ModelOutput dict\n",
        "                                g_seq = g_out.sequences[0]\n",
        "\n",
        "                            g_text = tokenizer.decode(g_seq, skip_special_tokens=True)\n",
        "                            generated_texts.append(g_text)\n",
        "                            try: avg_wer += wer(pseudo_text, g_text)\n",
        "                            except: avg_wer += 1.0\n",
        "\n",
        "                        for p, n in zip(params_to_noise, noise_cache):\n",
        "                            p.data.sub_(n)\n",
        "\n",
        "                    avg_wer = avg_wer / 3\n",
        "                    diversity = len(set(generated_texts))\n",
        "                    filter_metric = avg_wer * diversity\n",
        "\n",
        "                    sample_data = {\n",
        "                        'mel': mel.squeeze(0).cpu(),\n",
        "                        'labels': pred_ids.cpu(),\n",
        "                        'star_weights': torch.tensor(star_weights).cpu(),\n",
        "                        'filter_metric': filter_metric,\n",
        "                        'text': pseudo_text\n",
        "                    }\n",
        "                    chunk_data.append(sample_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save Chunk\n",
        "        if len(chunk_data) > 0:\n",
        "            temp_file = os.path.join(OUTPUT_DIR, f\"chunk_{chunk_idx}.pt\")\n",
        "            print(temp_file)\n",
        "            torch.save(chunk_data, temp_file)\n",
        "            temp_files.append(temp_file)\n",
        "\n",
        "        del chunk_data\n",
        "        gc.collect()\n",
        "\n",
        "    # --- MERGE & FILTER ---\n",
        "    print(\"\\nMerging and Filtering...\")\n",
        "    full_data = []\n",
        "    for f in temp_files:\n",
        "        if os.path.exists(f):\n",
        "            full_data.extend(torch.load(f, weights_only=False))\n",
        "            #os.remove(f)\n",
        "\n",
        "    if len(full_data) == 0:\n",
        "        print(\"CRITICAL: Final dataset is empty.\")\n",
        "        return\n",
        "\n",
        "    full_data.sort(key=lambda x: x['filter_metric'])\n",
        "    cutoff = int(len(full_data) * TOP_PERCENT)\n",
        "    final_data = full_data[:cutoff]\n",
        "\n",
        "    save_path = os.path.join(OUTPUT_DIR, \"train_final_ready.pt\")\n",
        "    torch.save(final_data, save_path)\n",
        "    print(f\"Success! Saved {len(final_data)} samples to {save_path}\")\n",
        "\n",
        "\n",
        "def train_with_star(model, train_dataset, device, num_epochs=10, batch_size=1, accum_steps=16):\n",
        "    \"\"\"\n",
        "    STAR Informed Fine-Tuning with Loss Re-weighting.\n",
        "    \"\"\"\n",
        "\n",
        "    # The original paper states that full fine-tuning performs better than freezing encoder\n",
        "    # therefore we'll be using full fine-tuning\n",
        "    # for param in model.model.encoder.parameters():\n",
        "    #    param.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100, reduction='none')\n",
        "\n",
        "    print(f\"Phase 2: Training on {len(train_dataset)} samples\")\n",
        "    print(f\"Config: BS={batch_size}, Accum={accum_steps}, Epochs={num_epochs}\")\n",
        "\n",
        "    # We explicitly define this to match the STAR loss splitting logic\n",
        "    n_prompt_toks = 4\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        random.shuffle(train_dataset)\n",
        "        epoch_loss = 0\n",
        "        step_count = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i in tqdm(range(0, len(train_dataset), batch_size)):\n",
        "            batch = train_dataset[i:i + batch_size]\n",
        "\n",
        "            # --- 1. Prepare Inputs ---\n",
        "            input_features = torch.cat([item['mel'].unsqueeze(0) for item in batch]).to(device)\n",
        "\n",
        "            # Labels\n",
        "            padded_labels = pad_sequence([item['labels'] for item in batch],\n",
        "                                         batch_first=True, padding_value=-100).to(device)\n",
        "\n",
        "            # STAR Weights\n",
        "            star_weights = [item['star_weights'].to(device) for item in batch]\n",
        "\n",
        "\n",
        "            # --- 2. Manual Shift for Causal Loss ---\n",
        "            # We slice manually to ensure alignment with our weights\n",
        "            decoder_input_ids = padded_labels[:, :-1]\n",
        "            targets = padded_labels[:, 1:]\n",
        "\n",
        "\n",
        "            # --- 3. Forward Pass ---\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                outputs = model(\n",
        "                    input_features=input_features,\n",
        "                    decoder_input_ids=decoder_input_ids\n",
        "                )\n",
        "                logits = outputs.logits\n",
        "\n",
        "                # Calculate Raw Loss (Unreduced)\n",
        "                # Shape: (Batch, Seq_Len)\n",
        "                loss_tensor = loss_fn(logits.permute(0, 2, 1), targets)\n",
        "\n",
        "                # --- 4. Apply STAR Weights (Informed Fine-Tuning) ---\n",
        "                # loss = (sum(prompt_loss) + sum(generated_loss * weights)) / total_len\n",
        "\n",
        "                final_loss = 0.0\n",
        "                valid_samples = 0\n",
        "\n",
        "                for b in range(len(batch)):\n",
        "                    sample_loss = loss_tensor[b]\n",
        "                    weights = star_weights[b]\n",
        "\n",
        "                    # Normalize weights\n",
        "                    if weights.mean() > 0:\n",
        "                        weights = weights / weights.mean()\n",
        "\n",
        "                    # Calculate indices\n",
        "                    # The prompt loss is the first (n_prompt_toks - 1) tokens\n",
        "                    prompt_loss_sum = torch.sum(sample_loss[:n_prompt_toks-1])\n",
        "\n",
        "                    # The generated loss is the rest\n",
        "                    gen_loss = sample_loss[n_prompt_toks-1:]\n",
        "\n",
        "                    # Truncate to matching length (handle padding/EOS mismatches safely)\n",
        "                    valid_len = min(len(gen_loss), len(weights))\n",
        "\n",
        "                    if valid_len > 0:\n",
        "                        # Apply Reweighting: Loss * STAR_Score\n",
        "                        weighted_gen_loss = gen_loss[:valid_len] * weights[:valid_len]\n",
        "\n",
        "                        # Combine: Prompt Loss (Unweighted) + Generated Loss (Weighted)\n",
        "                        total_sample_loss = prompt_loss_sum + torch.sum(weighted_gen_loss)\n",
        "\n",
        "                        # Normalize by total active tokens\n",
        "                        normalization_factor = (n_prompt_toks - 1) + valid_len\n",
        "                        final_loss += total_sample_loss / normalization_factor\n",
        "                        valid_samples += 1\n",
        "\n",
        "\n",
        "            # --- 5. Backward & Step ---\n",
        "            if valid_samples > 0:\n",
        "                # Average over batch\n",
        "                loss = final_loss / valid_samples\n",
        "\n",
        "                # Scale for accumulation\n",
        "                loss = loss / accum_steps\n",
        "                loss.backward()\n",
        "\n",
        "                epoch_loss += loss.item() * accum_steps\n",
        "\n",
        "            step_count += 1\n",
        "\n",
        "            if step_count % accum_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {epoch_loss / len(train_dataset):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def calculate_wer(reference, hypothesis):\n",
        "    return wer(reference, hypothesis)\n",
        "\n",
        "def evaluate(model, dataset, processor, feature_extractor, device):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    print(\" Phase 3: Evaluating \")\n",
        "\n",
        "    for item in tqdm(dataset):\n",
        "        # --- NEW: Process Raw Audio on the fly ---\n",
        "        audio_array = item['audio']['array']\n",
        "        sr = item['audio']['sampling_rate']\n",
        "\n",
        "        inputs = feature_extractor(audio_array, sampling_rate=sr, return_tensors=\"pt\")\n",
        "        mel = inputs.input_features.to(device)\n",
        "        # -----------------------------------------\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen_ids = model.generate(mel, max_new_tokens=225)\n",
        "            pred_text = processor.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "            preds.append(pred_text.lower())\n",
        "            targets.append(item['text'].lower())\n",
        "\n",
        "    return calculate_wer(targets, preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2434d8cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2434d8cb",
        "outputId": "a89bceef-5189-4a12-e5f4-b612fcfc8542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading STAR training data from data/star_process/train_final_ready.pt...\n",
            "Phase 2: Training on 6400 samples\n",
            "Config: BS=1, Accum=16, Epochs=10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:38<00:00, 18.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Avg Loss: 0.4467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:38<00:00, 18.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 - Avg Loss: 0.3012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:35<00:00, 19.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 - Avg Loss: 0.1839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:38<00:00, 18.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 - Avg Loss: 0.1183\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:39<00:00, 18.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 - Avg Loss: 0.0840\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:37<00:00, 18.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 - Avg Loss: 0.0628\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:40<00:00, 18.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 - Avg Loss: 0.0500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:43<00:00, 18.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 - Avg Loss: 0.0414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:54<00:00, 18.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 - Avg Loss: 0.0364\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6400/6400 [05:59<00:00, 17.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 - Avg Loss: 0.0328\n",
            "Model saved.\n",
            "\n",
            "Found 4 test sets.\n",
            "Evaluating in...\n",
            " Phase 3: Evaluating \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  7.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER for in: 42.29%\n",
            "Evaluating au...\n",
            " Phase 3: Evaluating \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER for au: 29.72%\n",
            "Evaluating sa...\n",
            " Phase 3: Evaluating \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER for sa: 45.31%\n",
            "Evaluating sg...\n",
            " Phase 3: Evaluating \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:12<00:00,  8.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER for sg: 46.30%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ================= MAIN EXECUTION =================\n",
        "\n",
        "# 1. Setup Model\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME)\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME, attn_implementation=\"eager\").to(DEVICE)\n",
        "\n",
        "# 2. Load Training Data\n",
        "train_file = os.path(INPUT_PT_FILE)\n",
        "\n",
        "# 3. Phase 1: Prepare STAR Data (Pseudo-labeling)\n",
        "star_data_file = os.path.join(STAR_PATH, \"train_final_ready.pt\")\n",
        "if os.path.exists(star_data_file):\n",
        "    print(f\"Loading STAR training data from {star_data_file}...\")\n",
        "    star_data = torch.load(star_data_file, weights_only=False)\n",
        "else:\n",
        "    if os.path.exists(train_file):\n",
        "        print(f\"Loading data from {train_file}...\")\n",
        "        star_data = prepare_star_dataset_chunked(model,INPUT_PT_FILE, processor, feature_extractor, tokenizer)\n",
        "\n",
        "# Clear cache to free VRAM\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# 4. Phase 2: Train\n",
        "trained_model = train_with_star(model, star_data, DEVICE)\n",
        "\n",
        "# Save Model\n",
        "torch.save(trained_model.state_dict(), \"star_model_final.pt\")\n",
        "print(\"Model saved.\")\n",
        "\n",
        "# 5. Phase 3: Evaluate on Accents\n",
        "# We look for all 'test_*.pt' files in the folder\n",
        "test_files = [f for f in os.listdir(TEST_PATH) if f.startswith(\"test_\") and f.endswith(\".pt\")]\n",
        "\n",
        "print(f\"\\nFound {len(test_files)} test sets.\")\n",
        "\n",
        "for t_file in test_files:\n",
        "    test_path = os.path.join(TEST_PATH, t_file)\n",
        "    test_dataset = torch.load(test_path, weights_only=False)\n",
        "\n",
        "    accent_name = t_file.replace(\"test_\", \"\").replace(\".pt\", \"\")\n",
        "    print(f\"Evaluating {accent_name}...\")\n",
        "\n",
        "    score = evaluate(trained_model, test_dataset, processor, feature_extractor, DEVICE)\n",
        "    print(f\"WER for {accent_name}: {score * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

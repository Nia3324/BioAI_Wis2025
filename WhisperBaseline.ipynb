{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "474bd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "from transformers import AutoFeatureExtractor, WhisperModel\n",
    "from transformers import LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch, torchaudio\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from jiwer import wer as calculate_wer\n",
    "import pickle\n",
    "from datasets import Dataset, Audio, Value\n",
    "import os, random\n",
    "from typing import Optional\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "import math\n",
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
    "from pathlib import Path\n",
    "import whisper\n",
    "import copy, heapq\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e8adabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-small', language='en', task='transcribe')  \n",
    "processor = WhisperProcessor.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "normalizer = EnglishTextNormalizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(\n",
    "    csv_path, \n",
    "    file_list_path,    \n",
    "    feature_extractor,\n",
    "    tokenizer=None\n",
    "):\n",
    "    df_text = pd.read_csv(csv_path)\n",
    "\n",
    "    with open(file_list_path, \"r\") as f:\n",
    "        file_paths = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        row = df_text[df_text[\"path\"] == file_path]\n",
    "        if row.empty:\n",
    "            print(f\"[WARN] No transcript found for: {file_path}\")\n",
    "            continue\n",
    "        text = str(row[\"sentence\"].values[0]).lower().strip()\n",
    "\n",
    "        # load audio\n",
    "        audio, sr = torchaudio.load(\"data/test_data/\" + file_path)\n",
    "        if sr != 16000:\n",
    "            audio = torchaudio.functional.resample(audio, sr, 16000)\n",
    "\n",
    "        # precompute features\n",
    "        feats = feature_extractor(\n",
    "            audio.squeeze(0).numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_features\"][0]\n",
    "\n",
    "        item = {\n",
    "            \"input_features\": feats,\n",
    "            \"text\": text\n",
    "        }\n",
    "\n",
    "        if tokenizer is not None:\n",
    "            labels = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                truncation=True\n",
    "            )[\"input_ids\"][0]\n",
    "            item[\"labels\"] = labels\n",
    "\n",
    "        dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def evaluate(model, dataset, tokenizer, device):\n",
    "    model.eval()\n",
    "    wer_scores = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        input_features = sample[\"input_features\"].unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(input_features)\n",
    "\n",
    "        transcription = tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )[0].lower().strip()\n",
    "\n",
    "        reference = sample[\"text\"].lower().strip()\n",
    "\n",
    "        wer_score = calculate_wer(reference, transcription)\n",
    "        wer_scores.append(wer_score)\n",
    "\n",
    "    return np.mean(wer_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261067f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero-shot = 0.26164054001554\n"
     ]
    }
   ],
   "source": [
    "test_data_csv = 'data/final_test.csv'\n",
    "test_audio_list = 'data/test_files.txt'  \n",
    "test_dataset = data_preparation(test_data_csv, test_audio_list, feature_extractor, tokenizer)\n",
    "hf_dataset = Dataset.from_list(test_dataset)\n",
    "torch.save(test_dataset, f'data/test.pt')\n",
    "device = 'cpu'\n",
    "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n",
    "model.eval()\n",
    "print(f'zero-shot WER = {evaluate(model, test_dataset, tokenizer, device)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

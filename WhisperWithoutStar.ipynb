{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "474bd5ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "474bd5ea",
        "outputId": "acf4b633-0b43-4ecd-ac84-03f5311f6074"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer openai-whisper torchcodec\n",
        "from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
        "from transformers import AutoFeatureExtractor, WhisperModel\n",
        "from transformers import LogitsProcessorList, EpsilonLogitsWarper\n",
        "\n",
        "from transformers import LlamaTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch, torchaudio\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "from jiwer import wer as calculate_wer\n",
        "import pickle\n",
        "from datasets import Dataset, Audio, Value\n",
        "import os, random\n",
        "from typing import Optional\n",
        "from whisper.normalizers import EnglishTextNormalizer\n",
        "import math\n",
        "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
        "from pathlib import Path\n",
        "import whisper\n",
        "import copy, heapq\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GjXWK3rxWnyE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjXWK3rxWnyE",
        "outputId": "8feae81d-134a-493d-8c58-9db83bd07d5c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7e8adabd",
      "metadata": {
        "id": "7e8adabd"
      },
      "outputs": [],
      "source": [
        "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
        "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
        "processor = WhisperProcessor.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "normalizer = EnglishTextNormalizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eef4c6c",
      "metadata": {
        "id": "2eef4c6c"
      },
      "outputs": [],
      "source": [
        "def train(model,train_dataset,val_dataset,tokenizer,feature_extractor,device,num_epochs=4,batch_size=1,learning_rate=1e-5,gradient_accumulation_steps=16):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    best_wer = float(\"inf\")\n",
        "    best_state_dict = copy.deepcopy(model.state_dict())\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        random.shuffle(train_dataset)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        for step, i in enumerate(range(0, len(train_dataset), batch_size)):\n",
        "            batch = train_dataset[i:i + batch_size]\n",
        "\n",
        "            input_features_batch = []\n",
        "            for item in batch:\n",
        "                audio_array = item[\"audio\"][\"array\"]\n",
        "                sampling_rate = item[\"audio\"][\"sampling_rate\"]\n",
        "                input_features_batch.append(feature_extractor(audio_array, sampling_rate=sampling_rate).input_features[0])\n",
        "\n",
        "            padded_input_features_dicts = [{\"input_features\": feat_tensor} for feat_tensor in input_features_batch]\n",
        "            feats_padded = feature_extractor.pad(padded_input_features_dicts, return_tensors=\"pt\")[\"input_features\"]\n",
        "\n",
        "            labels_batch = []\n",
        "            for item in batch:\n",
        "                tokenized_labels = tokenizer(item[\"text\"], add_special_tokens=True).input_ids\n",
        "                labels_batch.append(torch.tensor(tokenized_labels, dtype=torch.long))\n",
        "\n",
        "            labels_padded = pad_sequence(\n",
        "                labels_batch,\n",
        "                batch_first=True,\n",
        "                padding_value=tokenizer.pad_token_id\n",
        "            )\n",
        "            labels_padded[labels_padded == tokenizer.pad_token_id] = -100\n",
        "\n",
        "            feats_padded = feats_padded.to(device)\n",
        "            labels_padded = labels_padded.to(device)\n",
        "\n",
        "            outputs = model(input_features=feats_padded, labels=labels_padded)\n",
        "            loss = outputs.loss\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if step % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "        val_wer = evaluate(model, val_dataset, tokenizer, device)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation WER: {val_wer:.4f}\")\n",
        "\n",
        "        if val_wer < best_wer:\n",
        "            best_wer = val_wer\n",
        "            best_state_dict = copy.deepcopy(model.state_dict())\n",
        "            print(f\"  -> New best model (WER={best_wer:.4f}), saving in memory\")\n",
        "\n",
        "    model.load_state_dict(best_state_dict)\n",
        "    print(f\"Training done. Best WER: {best_wer:.4f}\")\n",
        "    return model\n",
        "\n",
        "def evaluate(model,dataset,tokenizer,device,batch_size=16):\n",
        "    model.eval()\n",
        "    wer_scores = []\n",
        "\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i:i + batch_size]\n",
        "\n",
        "        input_features_batch = []\n",
        "        for sample in batch:\n",
        "            audio_array = sample[\"audio\"][\"array\"]\n",
        "            sampling_rate = sample[\"audio\"][\"sampling_rate\"]\n",
        "            input_features_batch.append(feature_extractor(audio_array, sampling_rate=sampling_rate).input_features[0])\n",
        "\n",
        "        padded_input_features_dicts = [{\"input_features\": feat_tensor} for feat_tensor in input_features_batch]\n",
        "        feats_padded = feature_extractor.pad(padded_input_features_dicts, return_tensors=\"pt\")[\"input_features\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(feats_padded)\n",
        "\n",
        "        transcriptions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        for transcription, sample in zip(transcriptions, batch):\n",
        "            transcription = transcription.lower().strip()\n",
        "            reference = sample[\"text\"].lower().strip()\n",
        "            wer_score = calculate_wer(reference, transcription)\n",
        "            wer_scores.append(wer_score)\n",
        "\n",
        "    return np.mean(wer_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bYVMsLDFsiy",
      "metadata": {
        "id": "2bYVMsLDFsiy"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/data/final_train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/data/final_test.csv')\n",
        "df_accents = df_train['accents'].unique().tolist() + df_test['accents'].unique().tolist()\n",
        "list_accents = list(set(df_accents))\n",
        "list_accents = [list_accents[1]]\n",
        "del df_train\n",
        "del df_test\n",
        "del df_accents\n",
        "\n",
        "for accent in list_accents:\n",
        "    test_dataset = torch.load(f'/content/drive/MyDrive/data/{accent}/test.pt')\n",
        "    model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n",
        "    model.to(device)\n",
        "    print(f'zero-shot WER for {accent} = {evaluate(model, test_dataset, tokenizer, device)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fef37f3",
      "metadata": {
        "id": "7fef37f3"
      },
      "source": [
        "### Real-Labels dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "-Wv7SGzoVKtu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wv7SGzoVKtu",
        "outputId": "317e0963-482a-4409-a1a3-ae4f1e965566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Validation WER: 0.2626\n",
            "  -> New best model (WER=0.2626), saving in memory\n",
            "Epoch 2/4, Validation WER: 0.2676\n",
            "Epoch 3/4, Validation WER: 0.4248\n",
            "Epoch 4/4, Validation WER: 0.6084\n",
            "Training done. Best WER: 0.2626\n"
          ]
        }
      ],
      "source": [
        "training = torch.load(f'/content/drive/MyDrive/data/train.pt', weights_only=False)\n",
        "train_list = training\n",
        "validation = torch.load(f'/content/drive/MyDrive/data/validation.pt', weights_only=False)\n",
        "val_list = validation\n",
        "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')\n",
        "model.to(device)\n",
        "trained_model = train(model, train_list,val_list, tokenizer, feature_extractor, device)\n",
        "torch.save(trained_model.state_dict(), f'/content/drive/MyDrive/data/trained_model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

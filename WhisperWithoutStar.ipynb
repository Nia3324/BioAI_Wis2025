{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "474bd5ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "474bd5ea",
        "outputId": "5e351e17-19c9-426b-be62-179ecbde2d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer) (3.14.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install jiwer openai-whisper torchcodec\n",
        "from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
        "from transformers import AutoFeatureExtractor, WhisperModel\n",
        "from transformers import LlamaTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch, torchaudio\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "from jiwer import wer as calculate_wer\n",
        "import pickle\n",
        "from datasets import Dataset, Audio, Value\n",
        "import os, random\n",
        "from typing import Optional\n",
        "from whisper.normalizers import EnglishTextNormalizer\n",
        "import math\n",
        "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
        "from pathlib import Path\n",
        "import whisper\n",
        "import copy, heapq\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "GjXWK3rxWnyE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjXWK3rxWnyE",
        "outputId": "525735e5-7d60-4bc8-9a5d-7c46967049a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7e8adabd",
      "metadata": {
        "id": "7e8adabd"
      },
      "outputs": [],
      "source": [
        "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
        "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
        "processor = WhisperProcessor.from_pretrained('openai/whisper-small', language='en', task='transcribe')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "normalizer = EnglishTextNormalizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2eef4c6c",
      "metadata": {
        "id": "2eef4c6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, train_dataset, val_dataset, tokenizer, feature_extractor, device, num_epochs=2, batch_size=8, learning_rate=1e-5):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_wer = float(\"inf\")\n",
        "    best_state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        random.shuffle(train_dataset)\n",
        "\n",
        "        for i in range(0, len(train_dataset), batch_size):\n",
        "            batch = train_dataset[i:i + batch_size]\n",
        "\n",
        "            feats_list = [item[\"input_features\"].squeeze(0) for item in batch]\n",
        "            inputs = [{\"input_features\": f} for f in feats_list]\n",
        "            feats_padded = feature_extractor.pad(inputs, return_tensors=\"pt\")[\"input_features\"]\n",
        "\n",
        "            labels_list = [item[\"labels\"] for item in batch]\n",
        "            labels_padded = pad_sequence(labels_list,batch_first=True,padding_value=tokenizer.pad_token_id)\n",
        "            labels_padded[labels_padded == tokenizer.pad_token_id] = -100\n",
        "\n",
        "            feats_padded = feats_padded.to(device)\n",
        "            labels_padded = labels_padded.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_features=feats_padded, labels=labels_padded)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_wer = evaluate(model, val_dataset, tokenizer, device)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation WER: {val_wer:.4f}\")\n",
        "\n",
        "        if val_wer < best_wer:\n",
        "            best_wer = val_wer\n",
        "            best_state_dict = copy.deepcopy(model.state_dict())\n",
        "            print(f\"  -> New best model (WER={best_wer:.4f}), saving in memory\")\n",
        "\n",
        "    model.load_state_dict(best_state_dict)\n",
        "    print(f\"Training done. Best WER: {best_wer:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate(model, dataset, tokenizer, device, batch_size=4):\n",
        "    model.eval()\n",
        "    wer_scores = []\n",
        "\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i:i + batch_size]\n",
        "\n",
        "        input_features_list = [sample[\"input_features\"].squeeze(0) for sample in batch]\n",
        "        inputs = [{\"input_features\": f} for f in input_features_list]\n",
        "        feats_padded = feature_extractor.pad(inputs, return_tensors=\"pt\")[\"input_features\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(feats_padded)\n",
        "\n",
        "        transcriptions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        for transcription, sample in zip(transcriptions, batch):\n",
        "            transcription = transcription.lower().strip()\n",
        "            reference = sample[\"text\"].lower().strip()\n",
        "            wer_score = calculate_wer(reference, transcription)\n",
        "            wer_scores.append(wer_score)\n",
        "\n",
        "    return np.mean(wer_scores)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2bYVMsLDFsiy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bYVMsLDFsiy",
        "outputId": "875f6ba7-6c45-489f-db48-fe5ae7254934"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
            "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zero-shot WER for India and South Asia (India, Pakistan, Sri Lanka) = 0.18404442779442778\n",
            "zero-shot WER for Singaporean English = 0.5047703685203685\n",
            "zero-shot WER for Southern African (South Africa, Zimbabwe, Namibia) = 0.1972739482739483\n",
            "zero-shot WER for Australian English = 0.15097341547341547\n"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/data/final_train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/data/final_test.csv')\n",
        "df_accents = df_train['accents'].unique().tolist() + df_test['accents'].unique().tolist()\n",
        "list_accents = list(set(df_accents))\n",
        "del df_train\n",
        "del df_test\n",
        "del df_accents\n",
        "\n",
        "for accent in list_accents:\n",
        "    test_dataset = torch.load(f'/content/drive/MyDrive/data/{accent}/test.pt')\n",
        "    model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n",
        "    model.to(device)\n",
        "    print(f'zero-shot WER for {accent} = {evaluate(model, test_dataset, tokenizer, device)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fef37f3",
      "metadata": {
        "id": "7fef37f3"
      },
      "source": [
        "### Selft-trained dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "-Wv7SGzoVKtu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wv7SGzoVKtu",
        "outputId": "7e51866c-7941-4854-fda1-2b51f94610b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2, Validation WER: 0.1790\n",
            "  -> New best model (WER=0.1790), saving in memory\n",
            "Epoch 2/2, Validation WER: 0.1560\n",
            "  -> New best model (WER=0.1560), saving in memory\n",
            "Training done. Best WER: 0.1560\n",
            "Final WER after fine-tuning for India and South Asia (India, Pakistan, Sri Lanka) = 0.15600235875235877\n",
            "Epoch 1/2, Validation WER: 0.3677\n",
            "  -> New best model (WER=0.3677), saving in memory\n",
            "Epoch 2/2, Validation WER: 0.3936\n",
            "Training done. Best WER: 0.3677\n",
            "Final WER after fine-tuning for Singaporean English = 0.36765431790431796\n",
            "Epoch 1/2, Validation WER: 0.1733\n",
            "  -> New best model (WER=0.1733), saving in memory\n",
            "Epoch 2/2, Validation WER: 0.1624\n",
            "  -> New best model (WER=0.1624), saving in memory\n",
            "Training done. Best WER: 0.1624\n",
            "Final WER after fine-tuning for Southern African (South Africa, Zimbabwe, Namibia) = 0.16242465867465866\n",
            "Epoch 1/2, Validation WER: 0.1412\n",
            "  -> New best model (WER=0.1412), saving in memory\n",
            "Epoch 2/2, Validation WER: 0.1342\n",
            "  -> New best model (WER=0.1342), saving in memory\n",
            "Training done. Best WER: 0.1342\n",
            "Final WER after fine-tuning for Australian English = 0.1341775308025308\n"
          ]
        }
      ],
      "source": [
        "for accent in list_accents:\n",
        "    df_pseudo_training = torch.load(f'/content/drive/MyDrive/data/{accent}/df_pseudo_training.pt')\n",
        "    train_list = df_pseudo_training\n",
        "\n",
        "    test_dataset = torch.load(f'/content/drive/MyDrive/data/{accent}/test.pt')\n",
        "    test_list = test_dataset\n",
        "    model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\n",
        "    model.to(device)\n",
        "    trained_model = train(model, train_list, test_list, tokenizer, feature_extractor, device, batch_size=4)\n",
        "    print(f'Final WER after fine-tuning for {accent} = {evaluate(trained_model, test_dataset, tokenizer, device)}')\n",
        "    torch.save(trained_model.state_dict(), f'/content/drive/MyDrive/data/{accent}/self-trained_model.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
